{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnN8njvzswj3"
      },
      "outputs": [],
      "source": [
        "# Revisiting famous Research papers in field of Deep Learning / AI"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Revisiting the Breakthrough Research papers in Generative AI in Computer Vision\n",
        "\n",
        "# **Generative Adversarial Nets by Ian J. Goodfellow & co-authors**\n",
        "\n",
        "---> Generative Adversarial Networks (GANs) are a fundamental concept in the field of deep learning, particularly in the domain of generative modeling.\n",
        "\n",
        "**********************************\n",
        "\n",
        "**Introduction**:\n",
        "\n",
        "The paper addresses the challenge of generating data that is similar to a given dataset. Traditional methods for generating data (e.g., explicit probabilistic models) can be limited in their ability to capture complex data distributions. The authors propose a novel framework, GANs, which involves two neural networks, a generator and a discriminator, engaged in a competitive game.\n",
        "\n",
        "This introduces concept of adversarial networks in context of Machine Learning especially for Generative models.\n",
        "This is just a 2-player game setup where both sides compete with each other, in result improves themselves gradually.\n",
        "\n",
        "\n",
        "**The Setup:**\n",
        "\n",
        "  Generator (G): This network takes random noise (z) as input and tries to generate data (x) that resembles real data samples from a certain distribution (pdata). Essentially, it creates new data that should look like it's from the same source as the real data.\n",
        "\n",
        "  Discriminator (D): The discriminator takes either real data samples (x from pdata) or generated data samples (G(z)) as input and tries to distinguish between them. It essentially acts as a binary classifier, determining whether the input is from the real data distribution or the generated distribution.\n",
        "\n",
        "***************************************\n",
        "\n",
        "     ` minG maxD V(D, G) = Ex∼pdata(x)[log D(x)] + Ez∼pz(z)[log(1 - D(G(z)))]`\n",
        "\n",
        "\n",
        "Above equation in simpler terms:\n",
        "\n",
        " ->  The goal of the discriminator (D) is to maximize the probability of correctly classifying real data as real (log D(x)) and generated data as fake (log(1 - D(G(z)))).\n",
        "\n",
        "-> The goal of the generator (G) is to minimize the probability that the discriminator correctly classifies its generated data as fake (log(1 - D(G(z)))).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Training Process:**\n",
        "\n",
        "In practice, training the discriminator to completion in each iteration is computationally expensive and can lead to overfitting. Instead, the training process alternates between optimizing the discriminator for a few steps and then optimizing the generator for one step.\n",
        "\n",
        "The training algorithm (also called Adversarial training) is a two-step process:\n",
        "\n",
        "*  Update the discriminator (D) using real data samples and generated data samples, trying to improve its classification performance to become better at distinguishing between real and fake data..\n",
        "\n",
        "* Update the generator (G) to generate data that can \"fool\" the   discriminator, aiming to minimize the probability of the discriminator correctly classifying generated data as fake.\n",
        "\n",
        "**********************\n",
        "\n",
        "Later in the paper, It is also mentioned about the challenges while training the models.\n",
        "\n",
        "At the start of training, the generator might not create very convincing data, and the discriminator can easily tell that the generated samples are fake. In this case, the log(1 - D(G(z))) term can saturate, which means its gradient becomes too small to be useful for training the generator.\n",
        "\n",
        "To address this, instead of minimizing log(1 - D(G(z))), the generator can be trained to maximize log D(G(z)). This modification provides stronger gradients early in training and helps the generator improve more effectively.\n",
        "\n",
        "\n",
        "**Equilibrium and Convergence:**\n",
        "\n",
        "In an ideal scenario, this competition reaches an equilibrium point where the generator creates data that is almost indistinguishable from real data, and the discriminator struggles to classify it correctly.\n",
        "\n",
        "At this point, the generator has successfully captured the distribution of the training data.\n",
        "\n",
        "\n",
        "**Conclusions & future advancements in GANs:**\n",
        "This adversarial process has opened up new avenues in generative modeling and has had a significant impact on the field of deep learning.\n",
        "\n",
        "GANs have since been widely used for various applications, including image synthesis, style transfer, super-resolution, data augmentation, and more.\n",
        "\n",
        "\n",
        "They have also been extended to more complex architectures, like conditional GANs, progressive growth of GANs, Laplacian Pyramid GAN, Deep Convolutional GAN, WGAN, Dual GANs and more, which allow you to control the generated output more precisely.\n",
        "\n"
      ],
      "metadata": {
        "id": "u-gmTN-js7R-"
      }
    }
  ]
}